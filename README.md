# fixedpointdesc
Gradient Descent using Fixed Point Theorem

# How to Run

Run the command to do fixed point gradient descent on one dimensional interior point method cost function
```
python grad_desc_fixed_point_theory.py
```
Run the command to do Newton method on one dimensional interior point method cost function
```
python newton_method.py
```
Run the command to plot cost function values after each iteration of fixed point gradient descent and Newton method
```
python plot_two_methods.py
```

Run the command to do fixed point gradient descent on quadratic cost function
```
python grad_desc_fixed_point_theory_quadratic.py
```

Run the command to do fixed point gradient descent on non-convex cost function
```
python grad_desc_fixed_point_theory_doublehumps.py
```

# Experiment Results 

![Optimizing quadratic function](https://github.com/singkuangtan/fixedpointdesc/blob/main/gd.png)
![Optimizing interior point method function](https://github.com/singkuangtan/fixedpointdesc/blob/main/interior_point.png)
![Optimizing non-convex function](https://github.com/singkuangtan/fixedpointdesc/blob/main/nonconvex.png)

# Link
[FixedPointDesc paper link](https://vixra.org/abs/2302.0031)

[BSnet paper link](https://vixra.org/abs/2212.0193)

[BSautonet paper link](https://vixra.org/abs/2212.0208)

[BSautonet GitHub](https://github.com/singkuangtan/BSautonet)

[Discrete Markov Random Field Relaxation](https://vixra.org/abs/2112.0151)





